import os
import logging
from typing import List, Dict

# ‚úÖ ‡πÉ‡∏ä‡πâ Library ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏ô‡∏±‡∏î (LangChain)
# from langchain_chroma import Chroma
# from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from core.config import settings

# Setup Path
CURRENT_FILE_PATH = os.path.abspath(__file__)
BASE_DIR = os.path.dirname(os.path.dirname(CURRENT_FILE_PATH))  # Olympus-Agents Root
PERSIST_DIRECTORY = os.path.join(BASE_DIR, "chroma_db")

# ---------------------------------------------------------
# ‚ö° LAZY LOADING SETUP (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Time Out)
# ---------------------------------------------------------
# ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô None ‡∏Å‡πà‡∏≠‡∏ô (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î)
_VECTOR_DBS: Dict[str, any] = {}
_EMBEDDINGS = None

def get_vector_db(collection_name: str = "jira_knowledge"):
    """
    Init DB ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Collection (Lazy Load)
    """
    global _VECTOR_DBS, _EMBEDDINGS

    if collection_name not in _VECTOR_DBS:
        logging.info(f"‚è≥ Initializing Vector DB for collection: {collection_name}...")

        try:
            from langchain_chroma import Chroma
            from langchain_ollama import OllamaEmbeddings
        except ImportError as e:
            logging.error(f"‚ùå Critical Import Error: {e}")
            raise e

        # Init Embeddings ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠
        if _EMBEDDINGS is None:
            _EMBEDDINGS = OllamaEmbeddings(
                model=settings.EMBEDDING_MODEL,
                base_url=settings.OLLAMA_LOCAL_URL
            )

        # Init Chroma ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠ Collection
        _VECTOR_DBS[collection_name] = Chroma(
            collection_name=collection_name,
            embedding_function=_EMBEDDINGS,
            persist_directory=PERSIST_DIRECTORY
        )
        logging.info(f"‚úÖ Vector DB Ready for '{collection_name}'!")

    return _VECTOR_DBS[collection_name]

def add_ticket_to_vector(issue_key: str, summary: str, content: str):
    """
    Save ticket data to Vector DB.
    """
    # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏£‡∏á‡πÜ
    db = get_vector_db()

    logging.info(f"üß† VECTOR: Embedding ticket {issue_key}...")

    full_text = f"""
    Ticket: {issue_key}
    Summary: {summary}
    Knowledge: {content}
    """

    doc = Document(
        page_content=full_text,
        metadata={"issue_key": issue_key, "source": "jira"}
    )

    try:
        # ‡∏î‡∏∂‡∏á ID ‡πÄ‡∏Å‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
        existing_docs = db.get(where={"issue_key": issue_key})
        if existing_docs and existing_docs['ids']:
            db.delete(ids=existing_docs['ids'])
            logging.info(f"‚ôªÔ∏è Updated existing vector for {issue_key}")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Vector delete warning: {e}")

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° Vector ‡πÉ‡∏´‡∏°‡πà
    db.add_documents([doc])
    logging.info(f"‚úÖ VECTOR: Saved {issue_key} successfully.")

def search_vector_db(query: str, k: int = 4):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (Semantic Search)"""
    # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏£‡∏á‡πÜ
    db = get_vector_db()

    logging.info(f"üß† Semantic Searching for: '{query}'")

    results = db.similarity_search_with_score(query, k=k)

    if not results:
        return "‚ùå No relevant info found in Vector DB."

    parsed_results = []
    for doc, score in results:
        parsed_results.append(f"""
        --- MATCH (Score: {score:.2f}) ---
        Key: {doc.metadata.get('issue_key')}
        Content: {doc.page_content}
        -----------------------------------
        """)

    return "\n".join(parsed_results)

def add_robot_keyword_to_vector(library_name: str, keyword_name: str, arguments: str, doc_string: str):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Keyword ‡∏Ç‡∏≠‡∏á Robot Framework ‡∏•‡∏á‡πÉ‡∏ô Vector DB
    """
    db = get_vector_db("robot_framework_keywords")  # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Collection ‡πÉ‡∏´‡∏°‡πà

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á ID ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠ Library ‡πÅ‡∏•‡∏∞ Keyword
    doc_id = f"{library_name}.{keyword_name}".replace(" ", "_")

    # ‚úÇÔ∏è ‡∏à‡∏±‡∏î Format Text ‡∏ó‡∏µ‡πà AI ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô (Chunking)
    full_text = f"""
    Library: {library_name}
    Keyword: {keyword_name}
    Arguments: [ {arguments} ]
    Documentation: {doc_string}
    """

    doc = Document(
        page_content=full_text,
        metadata={
            "doc_id": doc_id,
            "library": library_name,
            "keyword": keyword_name,
            "source": "robot_libdoc"
        }
    )

    try:
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ Keyword ‡∏ô‡∏µ‡πâ‡πÑ‡∏´‡∏° ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏•‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
        existing = db.get(where={"doc_id": doc_id})
        if existing and existing['ids']:
            db.delete(ids=existing['ids'])
    except Exception as e:
        pass

    db.add_documents([doc])
    logging.info(f"‚úÖ VECTOR: Ingested Keyword '{keyword_name}' from {library_name}")

def search_robot_keywords(query: str, k: int = 5):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡πâ Arthemis ‡πÉ‡∏ä‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Keyword ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Code"""
    db = get_vector_db("robot_framework_keywords")
    results = db.similarity_search_with_score(query, k=k)

    if not results:
        return "‚ùå No matching keywords found. Use standard Python/Robot syntax."

    parsed_results = []
    for doc, score in results:
        parsed_results.append(f"--- MATCH (Score: {score:.2f}) ---\n{doc.page_content.strip()}\n")

    return "\n".join(parsed_results)