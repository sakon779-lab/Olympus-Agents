import requests
import json
import time
import logging
from core.config import settings

# ‚úÖ Setup Logger
logger = logging.getLogger("LLM_Client")

# ‚úÖ Import LangChain (Optional)
try:
    from langchain_ollama import ChatOllama
except ImportError:
    ChatOllama = None


def get_langchain_llm(temperature: float = 0):
    """
    ‚úÖ Factory Function: ‡∏™‡∏£‡πâ‡∏≤‡∏á LangChain Object
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SQL Agent ‡∏´‡∏£‡∏∑‡∏≠ Tool ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ LangChain Inteface
    """
    if ChatOllama is None:
        raise ImportError("‚ùå Please install 'langchain-ollama' to use this feature.")

    return ChatOllama(
        base_url=settings.OLLAMA_BASE_URL,
        model=settings.MODEL_NAME,
        temperature=temperature
    )


def query_qwen(messages: list, temperature: float = 0.0) -> str:
    """
    ‚úÖ Raw Function: ‡∏¢‡∏¥‡∏á Request ‡∏ï‡∏£‡∏á‡πÜ ‡∏û‡∏£‡πâ‡∏≠‡∏° Streaming output
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Conversation ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á Agent
    """
    # Construct Full URL
    api_url = f"{settings.OLLAMA_BASE_URL}/api/chat"

    print(f"\n[DEBUG] üì° Connecting to Ollama at {api_url}...", flush=True)
    print(f"[DEBUG] üß† Model: {settings.MODEL_NAME}", flush=True)

    payload = {
        "model": settings.MODEL_NAME,
        "messages": messages,
        "stream": True,
        "temperature": temperature,
        "options": {
            # "num_ctx": 4096,
            "num_ctx": 16000,
            "num_predict": -1
        }
    }

    try:
        print("[DEBUG] ‚è≥ Sending request... (Waiting for headers)", flush=True)

        # Timeout 120s ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ Model ‡∏Ñ‡∏¥‡∏î‡∏ô‡∏≤‡∏ô
        with requests.post(api_url, json=payload, stream=True, timeout=120) as response:
            if response.status_code != 200:
                error_msg = f"Error: Server returned {response.status_code} - {response.text}"
                logger.error(error_msg)
                return error_msg

            print(f"[DEBUG] ‚úÖ Connected! Status Code: {response.status_code}", flush=True)
            print("ü§ñ AI: ", end="", flush=True)

            full_content = ""

            for line in response.iter_lines():
                if line:
                    try:
                        body = json.loads(line)
                        content = body.get("message", {}).get("content", "")

                        if content:
                            print(content, end="", flush=True)
                            full_content += content

                        if body.get("done", False):
                            total_duration = body.get("total_duration", 0) / 1e9
                            tokens = body.get("eval_count", 0)
                            print(f"\n\n[DEBUG] üèÅ Done in {total_duration:.2f}s (Tokens: {tokens})")

                    except json.JSONDecodeError:
                        continue

            print("\n")
            return full_content

    except requests.exceptions.Timeout:
        logger.error("Connection Timed Out")
        return "Error: Timeout (Ollama took too long)"
    except requests.exceptions.ConnectionError:
        logger.error("Could not connect to Ollama")
        return "Error: Connection Refused (Is Ollama running?)"
    except Exception as e:
        logger.exception("Unexpected Error")
        return f"Error: {str(e)}"