import requests
import json
import time
import logging
import socket
from core.config import settings

# ‚úÖ Setup Logger
logger = logging.getLogger("LLM_Client")

def allowed_gai_family():
    return socket.AF_INET

# ‚úÖ Import LangChain (Optional)
try:
    from langchain_ollama import ChatOllama
except ImportError:
    ChatOllama = None


# def get_langchain_llm(temperature: float = 0):
#     """
#     ‚úÖ Factory Function: ‡∏™‡∏£‡πâ‡∏≤‡∏á LangChain Object
#     ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SQL Agent ‡∏´‡∏£‡∏∑‡∏≠ Tool ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ LangChain Inteface
#     """
#     if ChatOllama is None:
#         raise ImportError("‚ùå Please install 'langchain-ollama' to use this feature.")
#
#     return ChatOllama(
#         base_url=settings.OLLAMA_BASE_URL,
#         model=settings.MODEL_NAME,
#         temperature=temperature,
#
#         # üü¢ ‡∏¢‡πâ‡∏≤‡∏¢ config ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
#         # LangChain ‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡πÉ‡∏ô constructor ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
#         num_ctx=32000,
#         num_predict=-1,
#         keep_alive="60m",
#         request_timeout=600.0,  # üü¢ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
#         timeout=600.0,
#
#         # üü¢ ‡πÉ‡∏™‡πà options ‡∏¢‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ (LangChain ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)
#         options={
#             "num_ctx": 32000,
#             "num_predict": -1,
#             "temperature": temperature
#         }
#     )


def query_qwen(messages: list, temperature: float = 0.2) -> str:
    """
    ‚úÖ Raw Function: ‡∏¢‡∏¥‡∏á Request ‡∏ï‡∏£‡∏á‡πÜ ‡∏û‡∏£‡πâ‡∏≠‡∏° Streaming output
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Conversation ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á Agent
    """
    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ß‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î ---
    raw_payload = json.dumps(messages, ensure_ascii=False)
    char_count = len(raw_payload)
    est_tokens = char_count // 4  # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ 4 char = 1 token

    print(f"\n[DEBUG] üì¶ Outgoing Request Size:")
    print(f"        - Total Characters: {char_count:,}")
    print(f"        - Estimated Tokens: ~{est_tokens:,}")
    # ------------------

    # Construct Full URL
    api_url = f"{settings.OLLAMA_BASE_URL}/api/chat"

    print(f"\n[DEBUG] üì° Connecting to Ollama at {api_url}...", flush=True)
    print(f"[DEBUG] üß† Model: {settings.MODEL_NAME}", flush=True)

    payload = {
        "model": settings.MODEL_NAME,
        "messages": messages,
        "stream": True,
        "options": {
            # "num_ctx": 4096,
            "num_ctx": 64000,
            "num_predict": -1,
            "temperature": temperature,  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå (0 = ‡πÄ‡∏õ‡πä‡∏∞‡∏™‡∏∏‡∏î, 1 = ‡∏Å‡∏≤‡∏ß‡∏™‡∏∏‡∏î)
            "top_k": 40,  # 10 ‡∏ñ‡∏∂‡∏á 40 (‡∏õ‡∏Å‡∏ï‡∏¥ Ollama default ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 40 ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡∏¥‡πâ‡∏á‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 20-40 ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏ú‡∏•‡∏≠‡∏´‡∏¢‡∏¥‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÅ‡∏õ‡∏•‡∏Å‡πÜ ‡∏°‡∏≤‡πÉ‡∏ä‡πâ)
            "top_p": 0.85,  # (Nucleus): 0.1 ‡∏ñ‡∏∂‡∏á 0.5 (‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡πÜ ‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
            "repeat_penalty": 1.1
        }
    }

    max_retries = 3
    for attempt in range(max_retries):
        try:
            print("[DEBUG] ‚è≥ Sending request... (Waiting for headers)", flush=True)

            # Timeout 120s ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ Model ‡∏Ñ‡∏¥‡∏î‡∏ô‡∏≤‡∏ô
            with requests.post(api_url, json=payload, stream=True, timeout=120) as response:
                if response.status_code != 200:
                    error_msg = f"Error: Server returned {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return error_msg

                print(f"[DEBUG] ‚úÖ Connected! Status Code: {response.status_code}", flush=True)
                print("ü§ñ AI: ", end="", flush=True)

                full_content = ""

                for line in response.iter_lines():
                    if line:
                        try:
                            body = json.loads(line)
                            content = body.get("message", {}).get("content", "")

                            if content:
                                print(content, end="", flush=True)
                                full_content += content

                            if body.get("done", False):
                                total_duration = body.get("total_duration", 0) / 1e9
                                tokens = body.get("eval_count", 0)
                                print(f"\n\n[DEBUG] üèÅ Done in {total_duration:.2f}s (Tokens: {tokens})")

                        except json.JSONDecodeError:
                            continue

                print("\n")
                return full_content

        except requests.exceptions.ConnectionError:
            print(f"‚ö†Ô∏è Connection Refused. Server might be loading model. Retrying in 5s...", flush=True)
            time.sleep(5)  # ‚è≥ ‡∏£‡∏≠‡πÉ‡∏´‡πâ Server ‡∏ï‡∏∑‡πà‡∏ô (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
            continue  # ‡∏ß‡∏ô‡πÑ‡∏õ‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

        except requests.exceptions.Timeout:
            logger.error("Connection Timed Out")
            return "Error: Timeout (Ollama took too long)"

        except Exception as e:
            logger.exception("Unexpected Error")
            return f"Error: {str(e)}"

    return "Error: Failed to connect after retries"